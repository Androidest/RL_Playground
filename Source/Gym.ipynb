{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914f0f67-9006-4a9c-9364-d2939c69826e",
   "metadata": {},
   "source": [
    "# <font color='purple'>**Common**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485dcad9-ae6f-45fd-8eff-f16955afbb4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Instalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e36f3e1-5a60-490b-868a-c7f362ae4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym\n",
    "# !pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3700c7e-31cc-4b1b-bdea-d0edc018347e",
   "metadata": {},
   "source": [
    "## <font color='green'>Basics</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e69490aa-11a3-46fe-b3a6-9ba4ace6ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from numba import cuda\n",
    "import time\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "\n",
    "log_buffer = []\n",
    "\n",
    "def clearLog():\n",
    "    log_buffer = []\n",
    "\n",
    "def log(message):\n",
    "    if len(log_buffer) > 5:\n",
    "        log_buffer.pop(0)\n",
    "    log_buffer.append(message)\n",
    "    clear_output(wait=True)\n",
    "    for log in log_buffer:\n",
    "        print(log)\n",
    "\n",
    "def releaseMemory():\n",
    "    gc.collect()\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "\n",
    "def bn(x):\n",
    "    fx = layers.BatchNormalization()(x)\n",
    "    return fx\n",
    "\n",
    "def bn_relu(x, leaky=-1):\n",
    "    fx = layers.BatchNormalization()(x)\n",
    "    fx = relu(fx, leaky)\n",
    "    return fx\n",
    "\n",
    "def relu(x, leaky=-1):\n",
    "    if leaky == -1:\n",
    "        fx = layers.ReLU()(x)\n",
    "    else:\n",
    "        fx = layers.LeakyReLU(leaky)(x)\n",
    "    return fx\n",
    "\n",
    "def conv(x, filterNumb, kernel_size, strides=1, use_bias=True):\n",
    "    fx = layers.Conv2D(filterNumb, kernel_size, strides, padding='same', \n",
    "                    use_bias=use_bias, kernel_regularizer=l2(0.01))(x)\n",
    "    return fx\n",
    "\n",
    "def residual_block(x, filterNumb, kernel_size=3, poolStride=1):\n",
    "    shortcut = x\n",
    "    if poolStride != 1:\n",
    "        shortcut = conv(x, filterNumb, kernel_size=1, strides=poolStride)\n",
    "    \n",
    "    fx = conv(x, filterNumb, kernel_size=kernel_size, strides=poolStride)\n",
    "    fx = bn_relu(fx)\n",
    "    fx = conv(fx, filterNumb, kernel_size=kernel_size)\n",
    "    fx = layers.BatchNormalization()(fx)\n",
    "    fx = layers.Add()([fx, shortcut]) # skip\n",
    "    fx = relu(fx)\n",
    "    return fx\n",
    "\n",
    "def getEnvInputOutputShape(env):\n",
    "    env.reset()\n",
    "    img = env.render()\n",
    "    env.reset()\n",
    "    inShape_img = img[:,:,0].shape\n",
    "    inShape_vector = env.observation_space.shape\n",
    "    outShape = env.action_space.n\n",
    "    print(f'[getEnvInputOutputShape] inShape_img={inShape_img} inShape_vector={inShape_vector} outShape={outShape}')\n",
    "    return inShape_img, inShape_vector, outShape\n",
    "\n",
    "class Serializable:\n",
    "    def toJson(self, attrList=None, isInclude=True, file=None):\n",
    "        dict = {}\n",
    "        for key, value in self.__dict__.items():\n",
    "            if attrList != None:\n",
    "                if isInclude and key not in attrList:\n",
    "                    continue\n",
    "                elif not isInclude and key in attrList:\n",
    "                    continue\n",
    "            \n",
    "            if isinstance(value, np.ndarray):\n",
    "                value = value.tolist()\n",
    "            if isinstance(value, np.int32) or isinstance(value, np.int64) or isinstance(value, np.uint8):\n",
    "                value = int(value)\n",
    "            if isinstance(value, np.float32) or isinstance(value, np.float64):\n",
    "                value = float(value)\n",
    "            dict[key] = value\n",
    "        try:\n",
    "            if file is None:\n",
    "                return json.dumps(dict)\n",
    "            else:\n",
    "                json.dump(dict, file)\n",
    "        except Exception as e:\n",
    "            print(dict)\n",
    "            raise e\n",
    "\n",
    "    def fromJson(self, jsonStr):\n",
    "        dict = json.loads(jsonStr)\n",
    "        for key, value in dict.items():\n",
    "            if hasattr(self, key):  # Check if the object has the attribute\n",
    "                setattr(self, key, value)\n",
    "\n",
    "    def fromJsonFile(self, file):\n",
    "        dict = json.load(file)\n",
    "        for key, value in dict.items():\n",
    "            if hasattr(self, key):  # Check if the object has the attribute\n",
    "                setattr(self, key, value)\n",
    "\n",
    "class DQNBase:\n",
    "    def __init__(self, inputShape, outputShape, lr, loss_fn='mse'):\n",
    "        self.inputShape = inputShape\n",
    "        self.outputShape = outputShape\n",
    "        self.lr = lr\n",
    "        self.loss = -1\n",
    "        \n",
    "        inputs = layers.Input(shape=self.inputShape)\n",
    "        outputs = self.hiddenLayers(inputs)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "        if isinstance(loss_fn, str):\n",
    "            self.loss_fn = tf.keras.losses.get(loss_fn)\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        self.model.compile(optimizer=self.optimizer, loss=self.loss_fn,  metrics=['mae'])\n",
    "\n",
    "    def setLearningRate(self, lr):\n",
    "        if self.lr == lr:\n",
    "            return\n",
    "        self.lr = lr\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def clone(self):\n",
    "        cloned_instance = type(self)(self.inputShape, self.outputShape, self.model.optimizer.learning_rate.numpy())\n",
    "        cloned_instance.copyFrom(self)\n",
    "        return cloned_instance\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def trainOnBatch(self, batchX, targetY):\n",
    "        loss, _ = self.model.train_on_batch(batchX, targetY)\n",
    "        self.loss = loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict_on_batch(x)\n",
    "\n",
    "    def copyFrom(self, otherDQN):\n",
    "        self.loss = otherDQN.loss\n",
    "        self.model.set_weights(otherDQN.model.get_weights())\n",
    "\n",
    "    def hiddenLayers(self, inputs):\n",
    "        raise Exception('[DQN] hiddenLayers not implemented')\n",
    "\n",
    "class TemporalMemory(Serializable):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.array = np.empty(maxlen, dtype=object)  # Initialize with a numpy array of objects\n",
    "        self.head = 0\n",
    "        self.tail = 0\n",
    "        self.size = 0\n",
    "        self.cache_batch = None\n",
    "    \n",
    "    def toJson(self, file=None):\n",
    "        exclude = ['cache_batch']\n",
    "        return super().toJson(attrList=exclude, isInclude=False, file=file)\n",
    "    \n",
    "    def fromJson(self, json_str):\n",
    "        super().fromJson(json_str)\n",
    "        self.array = np.array(self.array)\n",
    "\n",
    "    def fromJsonFile(self, file):\n",
    "        super().fromJsonFile(file)\n",
    "        self.array = np.array(self.array)\n",
    "\n",
    "    def push(self, value):\n",
    "        if self.size < self.maxlen:\n",
    "            self.size += 1\n",
    "        \n",
    "        self.array[self.tail] = value\n",
    "        self.tail = (self.tail + 1) % self.maxlen\n",
    "\n",
    "    def sampleBatch(self, batchSize):\n",
    "        if self.size == self.maxlen:\n",
    "            array = self.array\n",
    "        else:\n",
    "            array = self.array[0:self.tail]\n",
    "        \n",
    "        if (batchSize <= len(array)):\n",
    "            batch = np.random.choice(array, batchSize, replace=False)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if self.cache_batch is None:\n",
    "            self.cache_batch = np.stack(batch, axis=0)\n",
    "        else:\n",
    "            np.stack(batch, axis=0, out=self.cache_batch)\n",
    "\n",
    "        return self.cache_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "class AgentBase(Serializable):\n",
    "    def reset(self):\n",
    "        pass\n",
    "        \n",
    "    def chooseAction(self, state_t):\n",
    "        print('[AgentBase] chooseAction not implemented')\n",
    "        return 0\n",
    "        \n",
    "    def train(self, batch, step):\n",
    "        pass\n",
    "\n",
    "class Plot(Serializable):\n",
    "    def __init__(self, size=[0, 10, 0, 10], winTitle=\"Plot\", xTitle=\"X Axis\", yTitle=\"Y Axis\"):\n",
    "        self.winTitle = winTitle\n",
    "        self.xTitle = xTitle\n",
    "        self.yTitle = yTitle\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        [self.xmin, self.xmax, self.ymin, self.ymax] = size\n",
    "    \n",
    "    def add(self, x, y):\n",
    "        self.xmin = min(self.xmin, x)\n",
    "        self.xmax = max(self.xmax, x)\n",
    "        self.ymin = min(self.ymin, y)\n",
    "        self.ymax = max(self.ymax, y)\n",
    "        self.X.append(x)\n",
    "        self.Y.append(y)\n",
    "\n",
    "    def show(self, msg):\n",
    "        clear_output(wait=True)\n",
    "        self.focus()\n",
    "        plt.plot(self.X, self.Y)\n",
    "        plt.text(self.xmin-(self.xmax-self.xmin)*0.2, self.ymin-(self.ymax-self.ymin)*0.2, msg, fontsize=10, color='red')\n",
    "        plt.show(block=False)\n",
    "\n",
    "    def focus(self):\n",
    "        plt.figure(hash(self))\n",
    "        plt.axis([self.xmin, self.xmax, self.ymin, self.ymax])\n",
    "        plt.title(self.winTitle)\n",
    "        plt.xlabel(self.xTitle)\n",
    "        plt.ylabel(self.yTitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4cd839",
   "metadata": {},
   "source": [
    "## <font color='green'>DQNAgent</font> Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68077f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(AgentBase):\n",
    "    def __init__(self, dqn, decayGamma=0.9, exploreRate=[0.01, 1, 0.9996], syncRate=10, eagerMode=False):\n",
    "        self.decayGamma = decayGamma # reward discount factor\n",
    "        [self.exploreRate_min, exploreRate_max, self.exploreRate_decay] = exploreRate\n",
    "        self.exploreRate = exploreRate_max\n",
    "        self.isTraining = True\n",
    "        self.syncRate = syncRate\n",
    "        self.waitToSync = 0\n",
    "        self.eagerMode = eagerMode\n",
    "\n",
    "        self.dqn_policy = dqn\n",
    "        self.dqn_target = dqn.clone()\n",
    "        self.numActions = dqn.outputShape\n",
    "\n",
    "        self.is_cache_init = False\n",
    "\n",
    "    def CopyBatchToCache(self, batch):\n",
    "        if not self.is_cache_init:\n",
    "            self.state_t_batch = np.stack(batch[:,0], axis=0)\n",
    "            self.state_t1_batch = np.stack(batch[:,3], axis=0)\n",
    "        \n",
    "        # SARS to numpy array\n",
    "        np.stack(batch[:,0], axis=0, out=self.state_t_batch)\n",
    "        self.action_t_batch = batch[:,1]\n",
    "        self.reward_t_batch = batch[:,2]\n",
    "        np.stack(batch[:,3], axis=0, out=self.state_t1_batch)\n",
    "        self.e_batch = batch[:,4]\n",
    "\n",
    "        if not self.eagerMode:\n",
    "            if not self.is_cache_init:\n",
    "                self.state_t_tensor = tf.Variable(self.state_t_batch, dtype=tf.float32)\n",
    "                self.action_t_tensor = tf.Variable(self.action_t_batch, dtype=tf.int32)\n",
    "                self.reward_t_tensor = tf.Variable(self.reward_t_batch, dtype=tf.float32)\n",
    "                self.state_t1_tensor = tf.Variable(self.state_t1_batch, dtype=tf.float32)\n",
    "                self.e_tensor = tf.Variable(self.e_batch, dtype=tf.float32)\n",
    "            else:\n",
    "                self.state_t_tensor.assign(self.state_t_batch)\n",
    "                self.action_t_tensor.assign(self.action_t_batch)\n",
    "                self.reward_t_tensor.assign(self.reward_t_batch)\n",
    "                self.state_t1_tensor.assign(self.state_t1_batch)\n",
    "                self.e_tensor.assign(self.e_batch)\n",
    "\n",
    "        self.is_cache_init = True\n",
    "        \n",
    "    def reset(self):\n",
    "        pass\n",
    "        \n",
    "    def chooseAction(self, state_t):\n",
    "        if self.isTraining and np.random.uniform(0, 1) < self.exploreRate:\n",
    "            action = random.randint(0, self.numActions-1)\n",
    "        else:\n",
    "            s = np.array([state_t])\n",
    "            actionsVal = self.dqn_policy.predict(s)\n",
    "            action = int(np.argmax(actionsVal, axis=1)[0])\n",
    "\n",
    "        if (self.exploreRate > self.exploreRate_min):\n",
    "            self.exploreRate = max(self.exploreRate * self.exploreRate_decay, self.exploreRate_min)\n",
    "\n",
    "        return action\n",
    "        \n",
    "    def train(self, batch, step):\n",
    "        if not self.isTraining:\n",
    "            return\n",
    "\n",
    "        self.trainOnBatch(batch)\n",
    "        # self.trainOnBatch_old(batch)\n",
    "\n",
    "        self.waitToSync += 1\n",
    "        if self.waitToSync >= self.syncRate:\n",
    "            self.waitToSync = 0\n",
    "            self.dqn_target.copyFrom(self.dqn_policy)\n",
    "\n",
    "    def trainOnBatch(self, batch):\n",
    "        self.CopyBatchToCache(batch)\n",
    "        \n",
    "        if self.eagerMode:\n",
    "            # train directly on numpy arrays\n",
    "            loss = self.trainOnTensor_eager(self.state_t_batch, self.action_t_batch, self.reward_t_batch, self.state_t1_batch, self.e_batch)\n",
    "        else:\n",
    "            # train on tensors\n",
    "            loss = self.trainOnTensor(self.state_t_tensor, self.action_t_tensor, self.reward_t_tensor, self.state_t1_tensor, self.e_tensor)\n",
    "\n",
    "        self.dqn_policy.loss = loss.numpy()\n",
    "\n",
    "    @tf.function\n",
    "    def trainOnTensor(self, state_t_batch, action_t_batch, reward_t_batch, state_t1_batch, e_batch):\n",
    "        # calculate the target Q value\n",
    "        Q_t1_batch = self.dqn_target.model(state_t1_batch, training=False)\n",
    "        next_action_value_batch = tf.reduce_max(Q_t1_batch, axis=1)\n",
    "        target_action_value_batch = reward_t_batch + e_batch * self.decayGamma * next_action_value_batch\n",
    "\n",
    "        # convert the current selected actions into onehot form\n",
    "        onehot_action_t_batch = tf.one_hot(action_t_batch, self.numActions, dtype=tf.float32)\n",
    "\n",
    "        # record operations performed on tensors for later gradients computations\n",
    "        with tf.GradientTape() as tape:\n",
    "            # calculate the policy Q value\n",
    "            Q_t_batch = self.dqn_policy.model(state_t_batch, training=True)\n",
    "            predict_action_value_batch = tf.reduce_sum(Q_t_batch * onehot_action_t_batch, axis=1)\n",
    "            \n",
    "            # calculate the losses of the two q values for the seleted actions\n",
    "            loss_value = self.dqn_policy.loss_fn(predict_action_value_batch, target_action_value_batch)\n",
    "\n",
    "        # calculate the gradients and update the dqn_policy model\n",
    "        gradients = tape.gradient(loss_value, self.dqn_policy.model.trainable_variables)\n",
    "        self.dqn_policy.optimizer.apply_gradients(zip(gradients, self.dqn_policy.model.trainable_variables))\n",
    "\n",
    "        # return the mean loss\n",
    "        return tf.reduce_mean(loss_value)\n",
    "\n",
    "    def trainOnTensor_eager(self, state_t_batch, action_t_batch, reward_t_batch, state_t1_batch, e_batch):\n",
    "        # calculate the target Q value\n",
    "        Q_t1_batch = self.dqn_target.model(state_t1_batch, training=False)\n",
    "        next_action_value_batch = tf.reduce_max(Q_t1_batch, axis=1)\n",
    "        target_action_value_batch = reward_t_batch + e_batch * self.decayGamma * next_action_value_batch\n",
    "\n",
    "        # convert the current selected actions into onehot form\n",
    "        onehot_action_t_batch = tf.one_hot(action_t_batch, self.numActions, dtype=tf.float32)\n",
    "\n",
    "        # record operations performed on tensors for later gradients computations\n",
    "        with tf.GradientTape() as tape:\n",
    "            # calculate the policy Q value\n",
    "            Q_t_batch = self.dqn_policy.model(state_t_batch, training=True)\n",
    "            predict_action_value_batch = tf.reduce_sum(Q_t_batch * onehot_action_t_batch, axis=1)\n",
    "            \n",
    "            # calculate the losses of the two q values for the seleted actions\n",
    "            loss_value = self.dqn_policy.loss_fn(predict_action_value_batch, target_action_value_batch)\n",
    "\n",
    "        # calculate the gradients and update the dqn_policy model\n",
    "        gradients = tape.gradient(loss_value, self.dqn_policy.model.trainable_variables)\n",
    "        self.dqn_policy.optimizer.apply_gradients(zip(gradients, self.dqn_policy.model.trainable_variables))\n",
    "\n",
    "        # return the mean loss\n",
    "        return tf.reduce_mean(loss_value)\n",
    "\n",
    "    def trainOnBatch_old(self, batch):\n",
    "        state_t_batch = np.stack(batch[:,0], axis=0)\n",
    "        action_t_batch = batch[:,1]\n",
    "        reward_t_batch = batch[:,2]\n",
    "        state_t1_batch = np.stack(batch[:,3], axis=0)\n",
    "        e_batch =  batch[:,4]\n",
    "\n",
    "        Q_t_target = self.dqn_policy.predict(state_t_batch)\n",
    "        Q_t1_batch = self.dqn_target.predict(state_t1_batch)\n",
    "        next_action_value_batch = np.max(Q_t1_batch, axis=1)\n",
    "        target_action_value_batch = reward_t_batch + e_batch * self.decayGamma * next_action_value_batch\n",
    "\n",
    "        for i in range(0, len(batch)):\n",
    "            action_t = action_t_batch[i]\n",
    "            Q_t_target[i, action_t] = target_action_value_batch[i]\n",
    "\n",
    "        self.dqn_policy.trainOnBatch(state_t_batch, Q_t_target)\n",
    "\n",
    "    def toData(self):\n",
    "        return [self.exploreRate, self.numActions, self.dqn_policy.lr]\n",
    "\n",
    "    def fromData(self, data):\n",
    "        [self.exploreRate, self.numActions, lr] = data\n",
    "        self.dqn_policy.setLearningRate(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a217f5-7cec-4151-a095-bb80240d756a",
   "metadata": {},
   "source": [
    "## <font color='green'>DeepQLearning</font> Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee9d890a-30ed-4959-94fb-1085f6ab2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "\n",
    "class DeepQLearning:\n",
    "    def __init__(self, env, agent, memSize=10000, batchSize=32, useImageInput=False):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.useImageInput = useImageInput\n",
    "        self.cache_frames = None\n",
    "        self.memory = TemporalMemory(memSize)\n",
    "        self.batchSize = batchSize\n",
    "        self.onInputImage = self.__onInputImage__\n",
    "        self.onEpisodeEnd = None\n",
    "        self.onStepEnd = None\n",
    "        self.onKeyPressed = None\n",
    "        self.lastStep = 0\n",
    "        \n",
    "    def __onInputImage__(self, img):\n",
    "        return img[:,:,0]\n",
    "    \n",
    "    def stackFramesToChannel(self, frames):\n",
    "        if self.cache_frames is None:\n",
    "            self.cache_frames = np.stack(frames, axis=-1)\n",
    "        else:\n",
    "            np.stack(frames, axis=-1, out=self.cache_frames)\n",
    "        return self.cache_frames.tolist()\n",
    "\n",
    "    def play(self, frameSkipping=1, steps=-1, fromLastStep=False, frameWaitTime=10):\n",
    "        episode = 1\n",
    "        step = 0 if not fromLastStep else self.lastStep\n",
    "\n",
    "        while step < steps or steps == -1:\n",
    "            # episode starts\n",
    "            state_t = None\n",
    "            state_t1_frames = []\n",
    "            reward_t = 0\n",
    "            gameEnd = False\n",
    "            score = 0\n",
    "            \n",
    "            self.agent.reset()\n",
    "            s_vector = self.env.reset()[0] \n",
    "            s_img = self.env.render()\n",
    "            s_frame = self.onInputImage(s_img) if self.useImageInput else s_vector\n",
    "            # duplicate the first frame 's_frame' to create a complete state\n",
    "            # complete state means stacking certain frames into a state\n",
    "            state_t = self.stackFramesToChannel([s_frame for _ in range(frameSkipping)])\n",
    "\n",
    "            # len(state_t1_frames) means frames skipped\n",
    "            while not gameEnd:\n",
    "                # choose a new action when state_t1_frames_frames is comsumed by state_t\n",
    "                if len(state_t1_frames) == 0: # len(state_t1_frames) == 0 means the new generated state_t1_frames became state_t\n",
    "                    action_t = self.agent.chooseAction(state_t)\n",
    "\n",
    "                # interact with the evironment using the current action, \n",
    "                # the current action repeats while frame skipping\n",
    "                s_vector, r_frame, terminated, truncated, info = self.env.step(action_t)\n",
    "                s_img = self.env.render()\n",
    "                s_frame = self.onInputImage(s_img) if self.useImageInput else s_vector\n",
    "\n",
    "                if frameWaitTime > 0:\n",
    "                    cv2.imshow('Game', s_img)\n",
    "                    key = cv2.waitKey(frameWaitTime)\n",
    "                    if key != None and self.onKeyPressed != None:\n",
    "                        self.onKeyPressed(key)\n",
    "\n",
    "                state_t1_frames.append(s_frame)\n",
    "                reward_t += r_frame\n",
    "                score += reward_t\n",
    "                gameEnd = terminated or truncated\n",
    "\n",
    "                # duplicate s into state_t1_frames to be a complete state when the game terminates\n",
    "                if gameEnd and len(state_t1_frames) < frameSkipping:\n",
    "                    for i in range(0, frameSkipping-len(state_t1_frames)):\n",
    "                        state_t1_frames.append(s_frame)\n",
    "\n",
    "                # train the agent with the result of iteraction after certain frames passed\n",
    "                if len(state_t1_frames) == frameSkipping: #len(state_t1_frames) == frameSkipping means new state is generated\n",
    "                    state_t1 = self.stackFramesToChannel(state_t1_frames)\n",
    "                    self.memory.push([state_t, action_t, reward_t, state_t1, 0 if terminated else 1])\n",
    "                    batch = self.memory.sampleBatch(self.batchSize)\n",
    "                    if batch is not None:\n",
    "                        self.agent.train(batch, step)\n",
    "                        \n",
    "                    state_t = state_t1\n",
    "                    state_t1_frames = []\n",
    "                    reward_t = 0\n",
    "                \n",
    "                if self.onStepEnd != None:\n",
    "                    self.onStepEnd(episode, step, score, terminated)  \n",
    "                step += 1\n",
    "                self.lastStep = step\n",
    "\n",
    "            # do something when one episode ends\n",
    "            if self.onEpisodeEnd != None:\n",
    "                self.onEpisodeEnd(episode, step, score)\n",
    "            episode += 1\n",
    "                \n",
    "        self.env.reset()\n",
    "        if frameWaitTime > 0:\n",
    "            cv2.waitKey(frameWaitTime)\n",
    "            cv2.destroyAllWindows()\n",
    "    \n",
    "    def save(self, filePath, extra_data=[]):\n",
    "        if filePath is None:\n",
    "            raise Exception('[DeepQLearning] save(filePath) filePath is None')\n",
    "        \n",
    "        mem_path = f'{filePath}.mem'\n",
    "        h5_path = f'{filePath}.h5'\n",
    "\n",
    "        os.makedirs(os.path.dirname(mem_path), exist_ok=True)\n",
    "        with open(mem_path, 'w') as f:\n",
    "            self.memory.toJson(f)\n",
    "        \n",
    "        # Save the model as an HDF5 file\n",
    "        self.agent.dqn_policy.model.save(h5_path)\n",
    "        # Add custom data to the same HDF5 file\n",
    "        with h5py.File(h5_path, 'a') as f:\n",
    "            f.create_dataset('custom_data/agent', data=self.agent.toData())\n",
    "            f.create_dataset('custom_data/lastStep', data=self.lastStep)\n",
    "            f.create_dataset('custom_data/extra_data', data=extra_data)\n",
    "\n",
    "    def load(self, filePath):\n",
    "        if filePath is None:\n",
    "            raise Exception('[DeepQLearning] load(filePath) filePath is None')\n",
    "\n",
    "        mem_path = f'{filePath}.mem'\n",
    "        h5_path = f'{filePath}.h5'\n",
    "        \n",
    "        if not os.path.exists(mem_path):\n",
    "            print(f'[DeepQLearning] fail to load mem file, file=\"{mem_path}\" does not exist')\n",
    "        else:\n",
    "            with open(mem_path, 'r') as f:            \n",
    "                self.memory.fromJsonFile(f)\n",
    "        \n",
    "        if not os.path.exists(h5_path):\n",
    "            print(f'[DeepQLearning] fail to load h5 file, file=\"{h5_path}\" does not exist')\n",
    "            return\n",
    "        else:\n",
    "            # Load the model as an HDF5 file\n",
    "            self.agent.dqn_policy.model.load_weights(h5_path)\n",
    "            self.agent.dqn_target = self.agent.dqn_policy.clone()\n",
    "            # To read the custom data back from the HDF5 file\n",
    "            with h5py.File(h5_path, 'r') as f:\n",
    "                self.agent.fromData(f['custom_data/agent'][()])\n",
    "                self.lastStep = int(f['custom_data/lastStep'][()])\n",
    "                return f['custom_data/extra_data'][()]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25792b9-3eac-4923-852a-7a7442fd1cca",
   "metadata": {},
   "source": [
    "## Test Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5534b50-c80b-40ee-9c0a-11f7baf7de15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AgentBase] chooseAction not implemented\n",
      "[AgentBase] chooseAction not implemented\n",
      "[AgentBase] chooseAction not implemented\n",
      "[getEnvInputOutputShape] inShape_img=(400, 600) inShape_vector=(4,) outShape=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((400, 600), (4,), 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "agent = AgentBase()\n",
    "b = DeepQLearning(env, agent)\n",
    "b.play(4, 2)\n",
    "\n",
    "getEnvInputOutputShape(env)\n",
    "\n",
    "# import cv2\n",
    "# env.reset()\n",
    "# img = env.render()\n",
    "# img = img[170:-80,:,0] \n",
    "# print(img.shape)\n",
    "# img = cv2.resize(img, (120, 80), interpolation=cv2.INTER_CUBIC) \n",
    "# print(img.dtype)\n",
    "\n",
    "# cv2.imshow('Image', img)\n",
    "# cv2.waitKey(1)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa973c1-d042-4309-a9e8-52c68f45e70d",
   "metadata": {},
   "source": [
    "# <font color='purple'>**Deep Q-Learning CartPole**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a183b8-1e78-457a-af3a-74f2332af290",
   "metadata": {},
   "source": [
    "## Test CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5820d-cd62-4f03-b82f-4dacc43d14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "state_shape = env.observation_space.shape\n",
    "action_count = env.action_space.n\n",
    "print(f'state_shape:{state_shape}, action_count:{action_count}')\n",
    "\n",
    "episodes = 5\n",
    "for e in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "    while not terminated and not truncated:\n",
    "        env.render()\n",
    "        action = random.choice([0, 1])\n",
    "        n_state, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(e, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dfa376",
   "metadata": {},
   "source": [
    "## <font color='green'>DQN_DenseResnet</font> Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cc9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_DenseResnet(DQNBase):\n",
    "    def hiddenLayers(self, inputs):\n",
    "        fx = layers.Flatten()(inputs)\n",
    "        fx = layers.Dense(128)(fx)\n",
    "        fx = bn(fx)\n",
    "        fx = relu(fx)\n",
    "        fx = self.dense_res(fx, 128)\n",
    "        fx = self.dense_res(fx, 128)\n",
    "\n",
    "        advantage = layers.Dense(self.outputShape, activation='linear')(fx)\n",
    "        value = layers.Dense(1, activation='linear')(fx)\n",
    "        q_values = value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))\n",
    "\n",
    "        return q_values\n",
    "\n",
    "    def dense_res(self, x, size):\n",
    "        fx = layers.Dense(size)(x)\n",
    "        fx = bn(fx)\n",
    "        fx = relu(fx)\n",
    "        fx = layers.Dense(size)(fx)\n",
    "        fx = bn(fx)\n",
    "        fx = layers.Add()([fx, x])\n",
    "        fx = relu(fx)\n",
    "        return fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3359d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== Data ==================\n",
    "# releaseMemory()\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "inputFrameCount = 4\n",
    "input_count = env.observation_space.shape[0]\n",
    "action_count = env.action_space.n\n",
    "inputShape=(inputFrameCount, input_count)\n",
    "outputShape=2\n",
    "lr=0.0006\n",
    "decayGamma=0.95\n",
    "exploreRate=[0.01, 1, 0.9996]\n",
    "steps = 50000\n",
    "syncRate=20\n",
    "batchSize=32\n",
    "memSize=20000\n",
    "\n",
    "loss = tf.keras.losses.Huber(delta=0.005)\n",
    "dqn_dense = DQN_DenseResnet(inputShape, outputShape, lr, loss)\n",
    "dqn_dense.summary()\n",
    "agent = DQNAgent(dqn_dense, decayGamma, exploreRate, syncRate)\n",
    "dq_rl = DeepQLearning(env, agent, memSize, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d64cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Training ==============\n",
    "maxScore = 0\n",
    "next_print_time = 0\n",
    "plot = Plot(size=[0,10000, 0, 500], xTitle='Steps', yTitle='Scores', winTitle='CartPole')\n",
    "filePath = 'CartPole/DenseResnet_data'\n",
    "data = dq_rl.load(filePath)\n",
    "\n",
    "if data is not None:\n",
    "    [plotJson] = data\n",
    "    plot.fromJson(plotJson)\n",
    "\n",
    "def onStepEnd(episode, step, score, terminated):\n",
    "    if step < 2000:\n",
    "        agent.dqn_policy.setLearningRate(0.000001)\n",
    "    elif step < 20000:\n",
    "        agent.dqn_policy.setLearningRate(0.0006)\n",
    "    elif step < 30000:\n",
    "        agent.dqn_policy.setLearningRate(0.0001)\n",
    "    else:\n",
    "        agent.dqn_policy.setLearningRate(0.00005)\n",
    "    \n",
    "    if step > 1000 and step % 1000 == 0:\n",
    "        dq_rl.save(filePath, [plot.toJson()])\n",
    "\n",
    "def onEpisodeEnd(episode, step, score):\n",
    "    global maxScore, next_print_time, plot\n",
    "    maxScore = max(score, maxScore)\n",
    "    plot.add(x=step, y=score)\n",
    "    if time.time() > next_print_time:\n",
    "        next_print_time = time.time() + 2\n",
    "        plot.show(f'[step={step}] score={score} max={maxScore} lr={agent.dqn_policy.lr} loss={agent.dqn_policy.loss} explor={agent.exploreRate}')\n",
    "    \n",
    "dq_rl.onStepEnd = onStepEnd\n",
    "dq_rl.onEpisodeEnd = onEpisodeEnd\n",
    "dq_rl.play(frameSkipping=inputFrameCount, steps=steps, fromLastStep=True, frameWaitTime=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d93c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Testing ==============\n",
    "agent.isTraining = False\n",
    "plot = Plot(size=[0,0, 0, 1300], xTitle='Steps', yTitle='Scores', winTitle='CartPole')\n",
    "def onEpisodeEnd(episode, step, score):\n",
    "    global maxScore\n",
    "    maxScore = max(score, maxScore)\n",
    "    plot.add(x=step, y=score)\n",
    "    plot.show(f'[step={step}] score={score} max={maxScore}')\n",
    "\n",
    "dq_rl.onStepEnd = None\n",
    "dq_rl.onEpisodeEnd = onEpisodeEnd\n",
    "dq_rl.play(frameSkipping=inputFrameCount, steps=10000, frameWaitTime=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb7d4e",
   "metadata": {},
   "source": [
    "## <font color='green'>DQN_ConvResnet</font> Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffef6a46-d753-456a-b39f-8f93a14334d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_ConvResnet(DQNBase):\n",
    "    def hiddenLayers(self, inputs):\n",
    "        fx = conv(inputs, 64, kernel_size=5, strides=2) # (32, 128) -> (16, 64)\n",
    "        fx = bn(fx)\n",
    "        fx = relu(fx)\n",
    "        fx = self.bottleneck(fx, neck_num=16, out_num=64, kernel_size=3)\n",
    "        fx = self.bottleneck(fx, neck_num=16, out_num=64, kernel_size=3)\n",
    "        fx = self.bottleneck(fx, neck_num=32, out_num=128, kernel_size=3, poolStride=2) # (16, 64) -> (8, 32)\n",
    "        fx = self.bottleneck(fx, neck_num=32, out_num=128, kernel_size=3)\n",
    "        fx = self.bottleneck(fx, neck_num=32, out_num=128, kernel_size=3)\n",
    "        fx = self.bottleneck(fx, neck_num=64, out_num=256, kernel_size=3, poolStride=2) # (8, 32) -> (4, 16)\n",
    "        fx = self.bottleneck(fx, neck_num=64, out_num=256, kernel_size=2)\n",
    "        fx = self.bottleneck(fx, neck_num=64, out_num=256, kernel_size=2)\n",
    "        fx = self.bottleneck(fx, neck_num=128, out_num=512, kernel_size=2, poolStride=2) # (4, 16) -> (2, 8)\n",
    "        fx = self.bottleneck(fx, neck_num=128, out_num=512, kernel_size=1)\n",
    "        fx = self.bottleneck(fx, neck_num=128, out_num=512, kernel_size=1)\n",
    "        fx = self.bottleneck(fx, neck_num=256, out_num=1024, kernel_size=2, poolStride=2) # (2, 8) -> (1, 4)\n",
    "        fx = layers.Flatten()(fx)\n",
    "        \n",
    "        fx = layers.Dense(128)(fx)\n",
    "        fx = bn(fx)\n",
    "        fx = relu(fx)\n",
    "        fx = self.dense_res(fx, 128)\n",
    "        fx = self.dense_res(fx, 128)\n",
    "\n",
    "        advantage = layers.Dense(self.outputShape, activation='linear')(fx)\n",
    "        value = layers.Dense(1, activation='linear')(fx)\n",
    "        q_values = value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))\n",
    "\n",
    "        return q_values\n",
    "\n",
    "    def residual_block(self, x, num, kernel_size=3, poolStride=1):\n",
    "        shortcut = x\n",
    "        if poolStride != 1:\n",
    "            shortcut = layers.AveragePooling2D(pool_size=poolStride, strides=poolStride, padding='same')(shortcut)\n",
    "        if num != shortcut.shape[-1]:\n",
    "            shortcut = conv(shortcut, num, kernel_size=1)\n",
    "            shortcut = bn(shortcut)\n",
    "        \n",
    "        fx = conv(x, num, kernel_size=kernel_size, strides=poolStride)\n",
    "        fx = bn_relu(fx)\n",
    "        fx = conv(fx, num, kernel_size=kernel_size)\n",
    "        fx = bn(fx)\n",
    "        fx = layers.Add()([fx, shortcut]) # skip\n",
    "        fx = relu(fx)\n",
    "        return fx\n",
    "        \n",
    "    def bottleneck(self, x, neck_num, out_num, kernel_size=3, poolStride=1):\n",
    "        shortcut = x\n",
    "        if poolStride != 1:\n",
    "            shortcut = layers.AveragePooling2D(pool_size=poolStride, strides=poolStride, padding='same')(shortcut)\n",
    "        if out_num != shortcut.shape[-1]:\n",
    "            shortcut = conv(shortcut, out_num, kernel_size=1)\n",
    "            shortcut = bn(shortcut)\n",
    "        \n",
    "        fx = conv(x, neck_num, kernel_size=1)\n",
    "        fx = bn_relu(fx)\n",
    "        fx = conv(fx, neck_num, kernel_size=kernel_size, strides=poolStride)\n",
    "        fx = bn_relu(fx)\n",
    "        fx = conv(fx, out_num, kernel_size=1)\n",
    "        fx = bn(fx)\n",
    "        fx = layers.Add()([fx, shortcut]) # skip\n",
    "        fx = relu(fx)\n",
    "        return fx\n",
    "        \n",
    "    def dense_res(self, x, num):\n",
    "        input_dim = x.shape[-1]\n",
    "        shortcut = x\n",
    "        \n",
    "        if input_dim != num:\n",
    "            shortcut = layers.Dense(num)(shortcut)\n",
    "            shortcut = bn(shortcut)\n",
    "            \n",
    "        fx = layers.Dense(num)(x)\n",
    "        fx = bn_relu(fx)\n",
    "        fx = layers.Dense(num)(fx)\n",
    "        fx = bn(fx)\n",
    "        fx = layers.Add()([fx, shortcut])\n",
    "        fx = relu(fx)\n",
    "        return fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eaa948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 128, 4)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 16, 64, 64)   6464        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 64, 64)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 16, 64, 64)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 64, 16)   1040        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 64, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 16, 64, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 64, 16)   2320        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 16, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 64, 64)   1088        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 64, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 16, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 16, 64, 64)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 64, 16)   1040        re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 64, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 16, 64, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 64, 16)   2320        re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 64, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 16, 64, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 64, 64)   1088        re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 64, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 64, 64)   0           batch_normalization_6[0][0]      \n",
      "                                                                 re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 16, 64, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 64, 32)   2080        re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 64, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 16, 64, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 32, 32)    9248        re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 32, 32)    128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 8, 32, 32)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 8, 32, 64)    0           re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 32, 128)   4224        re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 8, 32, 128)   8320        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 32, 128)   512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 32, 128)   512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 8, 32, 128)   0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 8, 32, 128)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 32, 32)    4128        re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 32, 32)    128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 8, 32, 32)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 32, 32)    9248        re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 32, 32)    128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 8, 32, 32)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 32, 128)   4224        re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 32, 128)   512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 32, 128)   0           batch_normalization_13[0][0]     \n",
      "                                                                 re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 8, 32, 128)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 32, 32)    4128        re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 32, 32)    128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 8, 32, 32)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 32, 32)    9248        re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 32, 32)    128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 8, 32, 32)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 32, 128)   4224        re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 32, 128)   512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 32, 128)   0           batch_normalization_16[0][0]     \n",
      "                                                                 re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 8, 32, 128)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 32, 64)    8256        re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 32, 64)    256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, 8, 32, 64)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 4, 16, 64)    36928       re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 4, 16, 64)    256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 4, 16, 64)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 4, 16, 128)   0           re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 4, 16, 256)   16640       re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 4, 16, 256)   33024       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 4, 16, 256)   1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 4, 16, 256)   1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 16, 256)   0           batch_normalization_20[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, 4, 16, 256)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 4, 16, 64)    16448       re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 4, 16, 64)    256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_19 (ReLU)                 (None, 4, 16, 64)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 4, 16, 64)    16448       re_lu_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 4, 16, 64)    256         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, 4, 16, 64)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 4, 16, 256)   16640       re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 4, 16, 256)   1024        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 16, 256)   0           batch_normalization_23[0][0]     \n",
      "                                                                 re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_21 (ReLU)                 (None, 4, 16, 256)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 4, 16, 64)    16448       re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 4, 16, 64)    256         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_22 (ReLU)                 (None, 4, 16, 64)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 4, 16, 64)    16448       re_lu_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 16, 64)    256         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_23 (ReLU)                 (None, 4, 16, 64)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 4, 16, 256)   16640       re_lu_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 16, 256)   1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 4, 16, 256)   0           batch_normalization_26[0][0]     \n",
      "                                                                 re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_24 (ReLU)                 (None, 4, 16, 256)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 4, 16, 128)   32896       re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 16, 128)   512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_25 (ReLU)                 (None, 4, 16, 128)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 2, 8, 128)    65664       re_lu_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 2, 8, 128)    512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_26 (ReLU)                 (None, 2, 8, 128)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 2, 8, 256)    0           re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 2, 8, 512)    66048       re_lu_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 2, 8, 512)    131584      average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 2, 8, 512)    2048        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 2, 8, 512)    2048        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 2, 8, 512)    0           batch_normalization_30[0][0]     \n",
      "                                                                 batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_27 (ReLU)                 (None, 2, 8, 512)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 2, 8, 128)    65664       re_lu_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 2, 8, 128)    512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_28 (ReLU)                 (None, 2, 8, 128)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 2, 8, 128)    16512       re_lu_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 2, 8, 128)    512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_29 (ReLU)                 (None, 2, 8, 128)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 2, 8, 512)    66048       re_lu_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 2, 8, 512)    2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 2, 8, 512)    0           batch_normalization_33[0][0]     \n",
      "                                                                 re_lu_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_30 (ReLU)                 (None, 2, 8, 512)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 2, 8, 128)    65664       re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 2, 8, 128)    512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_31 (ReLU)                 (None, 2, 8, 128)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 2, 8, 128)    16512       re_lu_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 2, 8, 128)    512         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_32 (ReLU)                 (None, 2, 8, 128)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 2, 8, 512)    66048       re_lu_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 2, 8, 512)    2048        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 2, 8, 512)    0           batch_normalization_36[0][0]     \n",
      "                                                                 re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_33 (ReLU)                 (None, 2, 8, 512)    0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 2, 8, 256)    131328      re_lu_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 2, 8, 256)    1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_34 (ReLU)                 (None, 2, 8, 256)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 1, 4, 256)    262400      re_lu_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 1, 4, 256)    1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_35 (ReLU)                 (None, 1, 4, 256)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 4, 512)    0           re_lu_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 1, 4, 1024)   263168      re_lu_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 1, 4, 1024)   525312      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 1, 4, 1024)   4096        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 1, 4, 1024)   4096        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 1, 4, 1024)   0           batch_normalization_40[0][0]     \n",
      "                                                                 batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_36 (ReLU)                 (None, 1, 4, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4096)         0           re_lu_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          524416      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 128)          512         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_37 (ReLU)                 (None, 128)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       re_lu_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_38 (ReLU)                 (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16512       re_lu_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 128)          0           batch_normalization_43[0][0]     \n",
      "                                                                 re_lu_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_39 (ReLU)                 (None, 128)          0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_40 (ReLU)                 (None, 128)          0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          16512       re_lu_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 128)          512         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 128)          0           batch_normalization_45[0][0]     \n",
      "                                                                 re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_41 (ReLU)                 (None, 128)          0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            258         re_lu_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLambda (None, 1)            0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            129         re_lu_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, 2)            0           dense_5[0][0]                    \n",
      "                                                                 tf.math.reduce_mean[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 2)            0           dense_6[0][0]                    \n",
      "                                                                 tf.math.subtract[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 2,667,587\n",
      "Trainable params: 2,650,819\n",
      "Non-trainable params: 16,768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#============ Training ==============\n",
    "# releaseMemory()\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "inputFrameCount = 4\n",
    "action_count = env.action_space.n\n",
    "imWidth = 128\n",
    "imHeight = 32\n",
    "inputShape=(imHeight, imWidth, inputFrameCount)\n",
    "outputShape=2\n",
    "lr=0.0006\n",
    "decayGamma=0.95\n",
    "exploreRate=[0.02, 1, 0.9999]\n",
    "steps = 500000\n",
    "syncRate=20\n",
    "batchSize=32\n",
    "memSize=20000\n",
    "\n",
    "loss = tf.keras.losses.Huber(delta=1.0)\n",
    "dqn_dense = DQN_ConvResnet(inputShape, outputShape, lr, loss)\n",
    "dqn_dense.summary()\n",
    "agent = DQNAgent(dqn_dense, decayGamma, exploreRate, syncRate)\n",
    "dq_rl = DeepQLearning(env, agent, memSize, batchSize, useImageInput=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a2a7575",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 1676532728 (char 1676532727)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m plot \u001b[38;5;241m=\u001b[39m Plot(size\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m300\u001b[39m], xTitle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSteps\u001b[39m\u001b[38;5;124m'\u001b[39m, yTitle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScores\u001b[39m\u001b[38;5;124m'\u001b[39m, winTitle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m filePath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole/DQN_ConvResnet_data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdq_rl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilePath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     [plotJson] \u001b[38;5;241m=\u001b[39m data\n",
      "Cell \u001b[1;32mIn[4], line 133\u001b[0m, in \u001b[0;36mDeepQLearning.load\u001b[1;34m(self, filePath)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(mem_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:            \n\u001b[1;32m--> 133\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromJsonFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(h5_path):\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[DeepQLearning] fail to load h5 file, file=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh5_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 183\u001b[0m, in \u001b[0;36mTemporalMemory.fromJsonFile\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromJsonFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, file):\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromJsonFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray)\n",
      "Cell \u001b[1;32mIn[2], line 112\u001b[0m, in \u001b[0;36mSerializable.fromJsonFile\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromJsonFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, file):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# Check if the object has the attribute\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\tf\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\tf\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\tf\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\tf\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 1676532728 (char 1676532727)"
     ]
    }
   ],
   "source": [
    "maxScore = 0\n",
    "next_print_time = 0\n",
    "plot = Plot(size=[0,10000, 0, 300], xTitle='Steps', yTitle='Scores', winTitle='CartPole')\n",
    "filePath = 'CartPole/DQN_ConvResnet_data'\n",
    "data = dq_rl.load(filePath)\n",
    "if data is not None:\n",
    "    [plotJson] = data\n",
    "    plot.fromJson(plotJson)\n",
    "\n",
    "def onInputImage(img):\n",
    "    return cv2.resize(img[170:-80,:,0], (imWidth, imHeight), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def onStepEnd(episode, step, score, terminated):\n",
    "    if step < 2000:\n",
    "        agent.dqn_policy.setLearningRate(0.00000001)\n",
    "    elif step < 10000:\n",
    "        agent.dqn_policy.setLearningRate(0.00001)\n",
    "    elif step < 50000:\n",
    "        agent.dqn_policy.setLearningRate(0.000005)\n",
    "    else:\n",
    "        agent.dqn_policy.setLearningRate(0.000003)\n",
    "\n",
    "    if step > 1000 and (step % 3000 == 0 or step % 100000 == 0):\n",
    "        dq_rl.save(filePath, [plot.toJson()])\n",
    "\n",
    "def onEpisodeEnd(episode, step, score):\n",
    "    global maxScore, next_print_time, plot\n",
    "    maxScore = max(score, maxScore)\n",
    "    if (episode % 10==0):\n",
    "        plot.add(x=step, y=maxScore)\n",
    "        maxScore = 0\n",
    "    if time.time() > next_print_time:\n",
    "        next_print_time = time.time() + 6\n",
    "        plot.show(f'[step={step}] score={score} lr={agent.dqn_policy.lr} loss={agent.dqn_policy.loss} explor={agent.exploreRate}')\n",
    "\n",
    "def onKeyPressed(key):\n",
    "    global lr\n",
    "    if key == ord('w'):\n",
    "        lr = lr * 10\n",
    "    elif key == ord('s'):\n",
    "        lr = lr * 0.1\n",
    "    agent.dqn_policy.setLearningRate(lr)\n",
    "\n",
    "dq_rl.onInputImage = onInputImage\n",
    "dq_rl.onStepEnd = onStepEnd\n",
    "dq_rl.onEpisodeEnd = onEpisodeEnd\n",
    "dq_rl.onKeyPressed = onKeyPressed\n",
    "dq_rl.play(frameSkipping=inputFrameCount, steps=steps, fromLastStep=True, frameWaitTime=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c286929",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.isTraining = False\n",
    "maxScore = 0\n",
    "sumScore = 0\n",
    "def onEpisodeEnd(episode, step, score):\n",
    "    global maxScore, sumScore\n",
    "    maxScore = max(score, maxScore)\n",
    "    sumScore += score\n",
    "    print(f'[step={step}] score={score} max={maxScore} mean={sumScore/episode}')\n",
    "\n",
    "dq_rl.onStepEnd = None\n",
    "dq_rl.onEpisodeEnd = onEpisodeEnd\n",
    "dq_rl.play(frameSkipping=inputFrameCount, steps=5000, frameWaitTime=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
