{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914f0f67-9006-4a9c-9364-d2939c69826e",
   "metadata": {},
   "source": [
    "# <font color='purple'>**Common**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485dcad9-ae6f-45fd-8eff-f16955afbb4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Instalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36f3e1-5a60-490b-868a-c7f362ae4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym\n",
    "# !pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3700c7e-31cc-4b1b-bdea-d0edc018347e",
   "metadata": {},
   "source": [
    "## <font color='green'>Basics</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69490aa-11a3-46fe-b3a6-9ba4ace6ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from numba import cuda\n",
    "import time\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "\n",
    "log_buffer = []\n",
    "\n",
    "def clearLog():\n",
    "    log_buffer = []\n",
    "\n",
    "def log(message):\n",
    "    if len(log_buffer) > 5:\n",
    "        log_buffer.pop(0)\n",
    "    log_buffer.append(message)\n",
    "    clear_output(wait=True)\n",
    "    for log in log_buffer:\n",
    "        print(log)\n",
    "\n",
    "def releaseMemory():\n",
    "    gc.collect()\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "\n",
    "def bn(x):\n",
    "    fx = layers.BatchNormalization()(x)\n",
    "    return fx\n",
    "\n",
    "def bn_relu(x, leaky=-1):\n",
    "    fx = layers.BatchNormalization()(x)\n",
    "    fx = relu(fx, leaky)\n",
    "    return fx\n",
    "\n",
    "def relu(x, leaky=-1):\n",
    "    if leaky == -1:\n",
    "        fx = layers.ReLU()(x)\n",
    "    else:\n",
    "        fx = layers.LeakyReLU(leaky)(x)\n",
    "    return fx\n",
    "\n",
    "def conv(x, filterNumb, kernel_size, strides=1, use_bias=True):\n",
    "    fx = layers.Conv2D(filterNumb, kernel_size, strides, padding='same', \n",
    "                    use_bias=use_bias, kernel_regularizer=l2(0.01))(x)\n",
    "    return fx\n",
    "\n",
    "def residual_block(x, filterNumb, kernel_size=3, poolStride=1):\n",
    "    shortcut = x\n",
    "    if poolStride != 1:\n",
    "        shortcut = conv(x, filterNumb, kernel_size=1, strides=poolStride)\n",
    "    \n",
    "    fx = conv(x, filterNumb, kernel_size=kernel_size, strides=poolStride)\n",
    "    fx = bn_relu(fx)\n",
    "    fx = conv(fx, filterNumb, kernel_size=kernel_size)\n",
    "    fx = layers.BatchNormalization()(fx)\n",
    "    fx = layers.Add()([fx, shortcut]) # skip\n",
    "    fx = relu(fx)\n",
    "    return fx\n",
    "\n",
    "def getEnvInputOutputShape(env):\n",
    "    env.reset()\n",
    "    img = env.render()\n",
    "    env.reset()\n",
    "    inShape_img = img[:,:,0].shape\n",
    "    inShape_vector = env.observation_space.shape\n",
    "    outShape = env.action_space.n\n",
    "    print(f'[getEnvInputOutputShape] inShape_img={inShape_img} inShape_vector={inShape_vector} outShape={outShape}')\n",
    "    return inShape_img, inShape_vector, outShape\n",
    "\n",
    "class Serializable:\n",
    "    def toJson(self, attrList=None, isInclude=True, file=None):\n",
    "        dict = {}\n",
    "        for key, value in self.__dict__.items():\n",
    "            if attrList != None:\n",
    "                if isInclude and key not in attrList:\n",
    "                    continue\n",
    "                elif not isInclude and key in attrList:\n",
    "                    continue\n",
    "            \n",
    "            if isinstance(value, np.ndarray):\n",
    "                value = value.tolist()\n",
    "            if isinstance(value, np.int32) or isinstance(value, np.int64) or isinstance(value, np.uint8):\n",
    "                value = int(value)\n",
    "            if isinstance(value, np.float32) or isinstance(value, np.float64):\n",
    "                value = float(value)\n",
    "            dict[key] = value\n",
    "        try:\n",
    "            if file is None:\n",
    "                return json.dumps(dict)\n",
    "            else:\n",
    "                json.dump(dict, file)\n",
    "        except Exception as e:\n",
    "            print(dict)\n",
    "            raise e\n",
    "\n",
    "    def fromJson(self, jsonStr):\n",
    "        dict = json.loads(jsonStr)\n",
    "        for key, value in dict.items():\n",
    "            if hasattr(self, key):  # Check if the object has the attribute\n",
    "                setattr(self, key, value)\n",
    "\n",
    "    def fromJsonFile(self, file):\n",
    "        dict = json.load(file)\n",
    "        for key, value in dict.items():\n",
    "            if hasattr(self, key):  # Check if the object has the attribute\n",
    "                setattr(self, key, value)\n",
    "\n",
    "class DQNBase:\n",
    "    def __init__(self, inputShape, outputShape, lr, loss_fn='mse'):\n",
    "        self.inputShape = inputShape\n",
    "        self.outputShape = outputShape\n",
    "        self.lr = lr\n",
    "        self.loss = -1\n",
    "        \n",
    "        inputs = layers.Input(shape=self.inputShape)\n",
    "        outputs = self.hiddenLayers(inputs)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "        if isinstance(loss_fn, str):\n",
    "            self.loss_fn = tf.keras.losses.get(loss_fn)\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        self.model.compile(optimizer=self.optimizer, loss=self.loss_fn,  metrics=['mae'])\n",
    "\n",
    "    def setLearningRate(self, lr):\n",
    "        if self.lr == lr:\n",
    "            return\n",
    "        self.lr = lr\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def clone(self):\n",
    "        cloned_instance = type(self)(self.inputShape, self.outputShape, self.model.optimizer.learning_rate.numpy())\n",
    "        cloned_instance.copyFrom(self)\n",
    "        return cloned_instance\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def trainOnBatch(self, batchX, targetY):\n",
    "        loss, _ = self.model.train_on_batch(batchX, targetY)\n",
    "        self.loss = loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict_on_batch(x)\n",
    "\n",
    "    def copyFrom(self, otherDQN):\n",
    "        self.loss = otherDQN.loss\n",
    "        self.model.set_weights(otherDQN.model.get_weights())\n",
    "\n",
    "    def hiddenLayers(self, inputs):\n",
    "        raise Exception('[DQN] hiddenLayers not implemented')\n",
    "    \n",
    "class TemporalMemory:\n",
    "    def __init__(self, maxlen, chunkSize=100):\n",
    "        self.chunkSize = chunkSize\n",
    "        self.maxlen = maxlen\n",
    "        self.array = np.empty(maxlen, dtype=object)  # Initialize with a numpy array of objects\n",
    "        self.tail = 0\n",
    "        self.size = 0\n",
    "        self.a = 0\n",
    "\n",
    "        chunkCount = math.ceil(self.maxlen/self.chunkSize)\n",
    "        self.savedChunks = [True] * chunkCount\n",
    "        self.cache_batch = None\n",
    "\n",
    "    def push(self, value):\n",
    "        if self.size < self.maxlen:\n",
    "            self.size += 1\n",
    "        \n",
    "        chunkIndex = math.floor(self.tail/self.chunkSize)\n",
    "        self.savedChunks[chunkIndex] = False\n",
    "\n",
    "        self.array[self.tail] = value\n",
    "        self.tail = (self.tail + 1) % self.maxlen\n",
    "\n",
    "    def sampleBatch(self, batchSize):\n",
    "        if self.size == self.maxlen:\n",
    "            array = self.array\n",
    "        else:\n",
    "            array = self.array[0:self.tail]\n",
    "        \n",
    "        if (batchSize <= len(array)):\n",
    "            batch = np.random.choice(array, batchSize, replace=False)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if self.cache_batch is None:\n",
    "            self.cache_batch = np.stack(batch, axis=0)\n",
    "        else:\n",
    "            np.stack(batch, axis=0, out=self.cache_batch)\n",
    "\n",
    "        return self.cache_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def save(self, filePath):\n",
    "        with h5py.File(filePath, 'a') as f:\n",
    "            if 'metadata' in f:\n",
    "                [chunkSize, maxlen, _, _] = f['metadata'][()]\n",
    "                if chunkSize != self.chunkSize:\n",
    "                    raise Exception('[TemporalMemory] currrent chunkSize = {self.chunkSize} and file chunkSize = {chunkSize} are different')\n",
    "                if maxlen != self.maxlen:\n",
    "                    raise Exception('[TemporalMemory] currrent maxlen = {self.maxlen} and file maxlen = {maxlen} are different')\n",
    "                f['metadata'][()] = [self.chunkSize, self.maxlen, self.tail, self.size]\n",
    "            else:\n",
    "                f['metadata'] = [self.chunkSize, self.maxlen, self.tail, self.size]\n",
    "\n",
    "            if 'array_chunks' in f:\n",
    "                chunks = f['array_chunks']\n",
    "                self.a = True\n",
    "            else:\n",
    "                chunks = f.create_dataset('array_chunks', shape=len(self.savedChunks), dtype=h5py.string_dtype(encoding='utf-8'), chunks=1)\n",
    "\n",
    "            for i in range(0, len(self.savedChunks)):\n",
    "                if not self.savedChunks[i]:\n",
    "                    self.savedChunks[i] = True\n",
    "                    start = i * self.chunkSize\n",
    "                    end = min(start + self.chunkSize, self.maxlen)\n",
    "                    chunk = self.array[start:end]\n",
    "                    jsonStr = json.dumps(chunk.tolist())\n",
    "                    chunks[i] = jsonStr\n",
    "\n",
    "    def load(self, filePath):\n",
    "        if not os.path.exists(filePath):\n",
    "            print(f'[TemporalMemory] fail to load filePath=\"{filePath}\", file does not exist')\n",
    "            return\n",
    "        \n",
    "        with h5py.File(filePath, 'r') as f:\n",
    "            [self.chunkSize, self.maxlen, self.tail, self.size] = f['metadata'][()]\n",
    "\n",
    "            if len(self.array) != self.maxlen:\n",
    "                self.array = np.empty(self.maxlen, dtype=object)\n",
    "\n",
    "            chunks = f['array_chunks']\n",
    "            self.savedChunks = [True] * len(chunks)\n",
    "            for i in range(len(chunks)):\n",
    "                jsonStr = chunks[i].decode('utf-8')\n",
    "                if jsonStr == '':\n",
    "                    continue\n",
    "                start = i * self.chunkSize\n",
    "                end = min(start + self.chunkSize, self.maxlen)\n",
    "                self.array[start:end] = json.loads(jsonStr)\n",
    "            a = self.array\n",
    "\n",
    "class AgentBase(Serializable):\n",
    "    def reset(self):\n",
    "        pass\n",
    "        \n",
    "    def chooseAction(self, state_t):\n",
    "        print('[AgentBase] chooseAction not implemented')\n",
    "        return 0\n",
    "        \n",
    "    def train(self, batch, step):\n",
    "        pass\n",
    "\n",
    "class Plot(Serializable):\n",
    "    def __init__(self, size=[0, 10, 0, 10], winTitle=\"Plot\", xTitle=\"X Axis\", yTitle=\"Y Axis\"):\n",
    "        self.winTitle = winTitle\n",
    "        self.xTitle = xTitle\n",
    "        self.yTitle = yTitle\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        [self.xmin, self.xmax, self.ymin, self.ymax] = size\n",
    "    \n",
    "    def add(self, x, y):\n",
    "        self.xmin = min(self.xmin, x)\n",
    "        self.xmax = max(self.xmax, x)\n",
    "        self.ymin = min(self.ymin, y)\n",
    "        self.ymax = max(self.ymax, y)\n",
    "        self.X.append(x)\n",
    "        self.Y.append(y)\n",
    "\n",
    "    def show(self, msg):\n",
    "        clear_output(wait=True)\n",
    "        self.focus()\n",
    "        plt.plot(self.X, self.Y)\n",
    "        plt.text(self.xmin-(self.xmax-self.xmin)*0.2, self.ymin-(self.ymax-self.ymin)*0.2, msg, fontsize=10, color='red')\n",
    "        plt.show(block=False)\n",
    "\n",
    "    def focus(self):\n",
    "        plt.figure(hash(self))\n",
    "        plt.axis([self.xmin, self.xmax, self.ymin, self.ymax])\n",
    "        plt.title(self.winTitle)\n",
    "        plt.xlabel(self.xTitle)\n",
    "        plt.ylabel(self.yTitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4cd839",
   "metadata": {},
   "source": [
    "## <font color='green'>DQNAgent</font> Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68077f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(AgentBase):\n",
    "    def __init__(self, dqn, decayGamma=0.9, exploreRate=[0.01, 1, 0.9996], syncRate=10, eagerMode=False):\n",
    "        self.decayGamma = decayGamma # reward discount factor\n",
    "        [self.exploreRate_min, exploreRate_max, self.exploreRate_decay] = exploreRate\n",
    "        self.exploreRate = exploreRate_max\n",
    "        self.isTraining = True\n",
    "        self.syncRate = syncRate\n",
    "        self.waitToSync = 0\n",
    "        self.eagerMode = eagerMode\n",
    "\n",
    "        self.dqn_policy = dqn\n",
    "        self.dqn_target = dqn.clone()\n",
    "        self.numActions = dqn.outputShape\n",
    "\n",
    "        self.is_cache_init = False\n",
    "\n",
    "    def CopyBatchToCache(self, batch):\n",
    "        if not self.is_cache_init:\n",
    "            self.state_t_batch = np.stack(batch[:,0], axis=0)\n",
    "            self.state_t1_batch = np.stack(batch[:,3], axis=0)\n",
    "        \n",
    "        # SARS to numpy array\n",
    "        np.stack(batch[:,0], axis=0, out=self.state_t_batch)\n",
    "        self.action_t_batch = batch[:,1]\n",
    "        self.reward_t_batch = batch[:,2]\n",
    "        np.stack(batch[:,3], axis=0, out=self.state_t1_batch)\n",
    "        self.e_batch = batch[:,4]\n",
    "\n",
    "        if not self.eagerMode:\n",
    "            if not self.is_cache_init:\n",
    "                self.state_t_tensor = tf.Variable(self.state_t_batch, dtype=tf.float32)\n",
    "                self.action_t_tensor = tf.Variable(self.action_t_batch, dtype=tf.int32)\n",
    "                self.reward_t_tensor = tf.Variable(self.reward_t_batch, dtype=tf.float32)\n",
    "                self.state_t1_tensor = tf.Variable(self.state_t1_batch, dtype=tf.float32)\n",
    "                self.e_tensor = tf.Variable(self.e_batch, dtype=tf.float32)\n",
    "            else:\n",
    "                self.state_t_tensor.assign(self.state_t_batch)\n",
    "                self.action_t_tensor.assign(self.action_t_batch)\n",
    "                self.reward_t_tensor.assign(self.reward_t_batch)\n",
    "                self.state_t1_tensor.assign(self.state_t1_batch)\n",
    "                self.e_tensor.assign(self.e_batch)\n",
    "\n",
    "        self.is_cache_init = True\n",
    "        \n",
    "    def reset(self):\n",
    "        pass\n",
    "        \n",
    "    def chooseAction(self, state_t):\n",
    "        if self.isTraining and np.random.uniform(0, 1) < self.exploreRate:\n",
    "            action = random.randint(0, self.numActions-1)\n",
    "        else:\n",
    "            s = np.array([state_t])\n",
    "            actionsVal = self.dqn_policy.predict(s)\n",
    "            action = int(np.argmax(actionsVal, axis=1)[0])\n",
    "\n",
    "        if (self.exploreRate > self.exploreRate_min):\n",
    "            self.exploreRate = max(self.exploreRate * self.exploreRate_decay, self.exploreRate_min)\n",
    "\n",
    "        return action\n",
    "        \n",
    "    def train(self, batch, step):\n",
    "        if not self.isTraining:\n",
    "            return\n",
    "\n",
    "        self.trainOnBatch(batch)\n",
    "        # self.trainOnBatch_old(batch)\n",
    "\n",
    "        self.waitToSync += 1\n",
    "        if self.waitToSync >= self.syncRate:\n",
    "            self.waitToSync = 0\n",
    "            self.dqn_target.copyFrom(self.dqn_policy)\n",
    "\n",
    "    def trainOnBatch(self, batch):\n",
    "        self.CopyBatchToCache(batch)\n",
    "        \n",
    "        if self.eagerMode:\n",
    "            # train directly on numpy arrays\n",
    "            loss = self.trainOnTensor_eager(self.state_t_batch, self.action_t_batch, self.reward_t_batch, self.state_t1_batch, self.e_batch)\n",
    "        else:\n",
    "            # train on tensors\n",
    "            loss = self.trainOnTensor(self.state_t_tensor, self.action_t_tensor, self.reward_t_tensor, self.state_t1_tensor, self.e_tensor)\n",
    "\n",
    "        self.dqn_policy.loss = loss.numpy()\n",
    "\n",
    "    @tf.function\n",
    "    def trainOnTensor(self, state_t_batch, action_t_batch, reward_t_batch, state_t1_batch, e_batch):\n",
    "        # calculate the target Q value\n",
    "        Q_t1_batch = self.dqn_target.model(state_t1_batch, training=False)\n",
    "        next_action_value_batch = tf.reduce_max(Q_t1_batch, axis=1)\n",
    "        target_action_value_batch = reward_t_batch + e_batch * self.decayGamma * next_action_value_batch\n",
    "\n",
    "        # convert the current selected actions into onehot form\n",
    "        onehot_action_t_batch = tf.one_hot(action_t_batch, self.numActions, dtype=tf.float32)\n",
    "\n",
    "        # record operations performed on tensors for later gradients computations\n",
    "        with tf.GradientTape() as tape:\n",
    "            # calculate the policy Q value\n",
    "            Q_t_batch = self.dqn_policy.model(state_t_batch, training=True)\n",
    "            predict_action_value_batch = tf.reduce_sum(Q_t_batch * onehot_action_t_batch, axis=1)\n",
    "            \n",
    "            # calculate the losses of the two q values for the seleted actions\n",
    "            loss_value = self.dqn_policy.loss_fn(predict_action_value_batch, target_action_value_batch)\n",
    "\n",
    "        # calculate the gradients and update the dqn_policy model\n",
    "        gradients = tape.gradient(loss_value, self.dqn_policy.model.trainable_variables)\n",
    "        self.dqn_policy.optimizer.apply_gradients(zip(gradients, self.dqn_policy.model.trainable_variables))\n",
    "\n",
    "        # return the mean loss\n",
    "        return tf.reduce_mean(loss_value)\n",
    "\n",
    "    def trainOnTensor_eager(self, state_t_batch, action_t_batch, reward_t_batch, state_t1_batch, e_batch):\n",
    "        # calculate the target Q value\n",
    "        Q_t1_batch = self.dqn_target.model(state_t1_batch, training=False)\n",
    "        next_action_value_batch = tf.reduce_max(Q_t1_batch, axis=1)\n",
    "        target_action_value_batch = reward_t_batch + e_batch * self.decayGamma * next_action_value_batch\n",
    "\n",
    "        # convert the current selected actions into onehot form\n",
    "        onehot_action_t_batch = tf.one_hot(action_t_batch, self.numActions, dtype=tf.float32)\n",
    "\n",
    "        # record operations performed on tensors for later gradients computations\n",
    "        with tf.GradientTape() as tape:\n",
    "            # calculate the policy Q value\n",
    "            Q_t_batch = self.dqn_policy.model(state_t_batch, training=True)\n",
    "            predict_action_value_batch = tf.reduce_sum(Q_t_batch * onehot_action_t_batch, axis=1)\n",
    "            \n",
    "            # calculate the losses of the two q values for the seleted actions\n",
    "            loss_value = self.dqn_policy.loss_fn(predict_action_value_batch, target_action_value_batch)\n",
    "\n",
    "        # calculate the gradients and update the dqn_policy model\n",
    "        gradients = tape.gradient(loss_value, self.dqn_policy.model.trainable_variables)\n",
    "        self.dqn_policy.optimizer.apply_gradients(zip(gradients, self.dqn_policy.model.trainable_variables))\n",
    "\n",
    "        # return the mean loss\n",
    "        return tf.reduce_mean(loss_value)\n",
    "\n",
    "    def trainOnBatch_old(self, batch):\n",
    "        state_t_batch = np.stack(batch[:,0], axis=0)\n",
    "        action_t_batch = batch[:,1]\n",
    "        reward_t_batch = batch[:,2]\n",
    "        state_t1_batch = np.stack(batch[:,3], axis=0)\n",
    "        e_batch =  batch[:,4]\n",
    "\n",
    "        Q_t_target = self.dqn_policy.predict(state_t_batch)\n",
    "        Q_t1_batch = self.dqn_target.predict(state_t1_batch)\n",
    "        next_action_value_batch = np.max(Q_t1_batch, axis=1)\n",
    "        target_action_value_batch = reward_t_batch + e_batch * self.decayGamma * next_action_value_batch\n",
    "\n",
    "        for i in range(0, len(batch)):\n",
    "            action_t = action_t_batch[i]\n",
    "            Q_t_target[i, action_t] = target_action_value_batch[i]\n",
    "\n",
    "        self.dqn_policy.trainOnBatch(state_t_batch, Q_t_target)\n",
    "\n",
    "    def toData(self):\n",
    "        return [self.exploreRate, self.numActions, self.dqn_policy.lr]\n",
    "\n",
    "    def fromData(self, data):\n",
    "        [self.exploreRate, self.numActions, lr] = data\n",
    "        self.dqn_policy.setLearningRate(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a217f5-7cec-4151-a095-bb80240d756a",
   "metadata": {},
   "source": [
    "## <font color='green'>DeepQLearning</font> Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d890a-30ed-4959-94fb-1085f6ab2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "\n",
    "class DeepQLearning:\n",
    "    def __init__(self, env, agent, memSize=10000, batchSize=32, useImageInput=False):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.useImageInput = useImageInput\n",
    "        self.cache_frames = None\n",
    "        self.memory = TemporalMemory(memSize)\n",
    "        self.batchSize = batchSize\n",
    "        self.onInputImage = self.__onInputImage__\n",
    "        self.onEpisodeEnd = None\n",
    "        self.onStepEnd = None\n",
    "        self.onKeyPressed = None\n",
    "        self.lastStep = 0\n",
    "        \n",
    "    def __onInputImage__(self, img):\n",
    "        return img[:,:,0]\n",
    "    \n",
    "    def stackFramesToChannel(self, frames):\n",
    "        if self.cache_frames is None:\n",
    "            self.cache_frames = np.stack(frames, axis=-1)\n",
    "        else:\n",
    "            np.stack(frames, axis=-1, out=self.cache_frames)\n",
    "        return self.cache_frames.tolist()\n",
    "\n",
    "    def play(self, frameSkipping=1, steps=-1, fromLastStep=False, frameWaitTime=10):\n",
    "        episode = 1\n",
    "        step = 0 if not fromLastStep else self.lastStep\n",
    "\n",
    "        while step < steps or steps == -1:\n",
    "            # episode starts\n",
    "            state_t = None\n",
    "            state_t1_frames = []\n",
    "            reward_t = 0\n",
    "            gameEnd = False\n",
    "            score = 0\n",
    "            \n",
    "            self.agent.reset()\n",
    "            s_vector = self.env.reset()[0] \n",
    "            s_img = self.env.render()\n",
    "            s_frame = self.onInputImage(s_img) if self.useImageInput else s_vector\n",
    "            # duplicate the first frame 's_frame' to create a complete state\n",
    "            # complete state means stacking certain frames into a state\n",
    "            state_t = self.stackFramesToChannel([s_frame for _ in range(frameSkipping)])\n",
    "\n",
    "            # len(state_t1_frames) means frames skipped\n",
    "            while not gameEnd:\n",
    "                # choose a new action when state_t1_frames_frames is comsumed by state_t\n",
    "                if len(state_t1_frames) == 0: # len(state_t1_frames) == 0 means the new generated state_t1_frames became state_t\n",
    "                    action_t = self.agent.chooseAction(state_t)\n",
    "\n",
    "                # interact with the evironment using the current action, \n",
    "                # the current action repeats while frame skipping\n",
    "                s_vector, r_frame, terminated, truncated, info = self.env.step(action_t)\n",
    "                s_img = self.env.render()\n",
    "                s_frame = self.onInputImage(s_img) if self.useImageInput else s_vector\n",
    "\n",
    "                if frameWaitTime > 0:\n",
    "                    cv2.imshow('Game', s_img)\n",
    "                    key = cv2.waitKey(frameWaitTime)\n",
    "                    if key != None and self.onKeyPressed != None:\n",
    "                        self.onKeyPressed(key)\n",
    "\n",
    "                state_t1_frames.append(s_frame)\n",
    "                reward_t += r_frame\n",
    "                score += reward_t\n",
    "                gameEnd = terminated or truncated\n",
    "\n",
    "                # duplicate s into state_t1_frames to be a complete state when the game terminates\n",
    "                if gameEnd and len(state_t1_frames) < frameSkipping:\n",
    "                    for i in range(0, frameSkipping-len(state_t1_frames)):\n",
    "                        state_t1_frames.append(s_frame)\n",
    "\n",
    "                # train the agent with the result of iteraction after certain frames passed\n",
    "                if len(state_t1_frames) == frameSkipping: #len(state_t1_frames) == frameSkipping means new state is generated\n",
    "                    state_t1 = self.stackFramesToChannel(state_t1_frames)\n",
    "                    self.memory.push([state_t, action_t, reward_t, state_t1, 0 if terminated else 1])\n",
    "                    batch = self.memory.sampleBatch(self.batchSize)\n",
    "                    if batch is not None:\n",
    "                        self.agent.train(batch, step)\n",
    "                        \n",
    "                    state_t = state_t1\n",
    "                    state_t1_frames = []\n",
    "                    reward_t = 0\n",
    "                \n",
    "                if self.onStepEnd != None:\n",
    "                    self.onStepEnd(episode, step, score, terminated)  \n",
    "                step += 1\n",
    "                self.lastStep = step\n",
    "\n",
    "            # do something when one episode ends\n",
    "            if self.onEpisodeEnd != None:\n",
    "                self.onEpisodeEnd(episode, step, score)\n",
    "            episode += 1\n",
    "                \n",
    "        self.env.reset()\n",
    "        if frameWaitTime > 0:\n",
    "            cv2.waitKey(frameWaitTime)\n",
    "            cv2.destroyAllWindows()\n",
    "    \n",
    "    def save(self, filePath, extra_data=[]):\n",
    "        if filePath is None:\n",
    "            raise Exception('[DeepQLearning] save(filePath) filePath is None')\n",
    "        \n",
    "        mem_path_old = f'{filePath}_mem_old.h5'\n",
    "        mem_path = f'{filePath}_mem.h5'\n",
    "        h5_path = f'{filePath}.h5'\n",
    "\n",
    "        if os.path.exists(mem_path_old):\n",
    "            os.remove(mem_path_old)\n",
    "        if os.path.exists(mem_path):\n",
    "            os.rename(mem_path, mem_path_old)\n",
    "        else:\n",
    "            os.makedirs(os.path.dirname(mem_path), exist_ok=True)\n",
    "        \n",
    "        self.memory.save(mem_path)\n",
    "        \n",
    "        # Save the model as an HDF5 file\n",
    "        self.agent.dqn_policy.model.save(h5_path)\n",
    "        # Add custom data to the same HDF5 file\n",
    "        with h5py.File(h5_path, 'a') as f:\n",
    "            f.create_dataset('custom_data/agent', data=self.agent.toData())\n",
    "            f.create_dataset('custom_data/lastStep', data=self.lastStep)\n",
    "            f.create_dataset('custom_data/extra_data', data=extra_data)\n",
    "\n",
    "    def load(self, filePath):\n",
    "        if filePath is None:\n",
    "            raise Exception('[DeepQLearning] load(filePath) filePath is None')\n",
    "\n",
    "        mem_path = f'{filePath}_mem.h5'\n",
    "        h5_path = f'{filePath}.h5'\n",
    "        \n",
    "        if not os.path.exists(mem_path):\n",
    "            print(f'[DeepQLearning] fail to load mem file, file=\"{mem_path}\" does not exist')\n",
    "        else:\n",
    "            self.memory.load(mem_path)          \n",
    "                \n",
    "        \n",
    "        if not os.path.exists(h5_path):\n",
    "            print(f'[DeepQLearning] fail to load h5 file, file=\"{h5_path}\" does not exist')\n",
    "            return\n",
    "        else:\n",
    "            # Load the model as an HDF5 file\n",
    "            self.agent.dqn_policy.model.load_weights(h5_path)\n",
    "            self.agent.dqn_target = self.agent.dqn_policy.clone()\n",
    "            # To read the custom data back from the HDF5 file\n",
    "            with h5py.File(h5_path, 'r') as f:\n",
    "                self.agent.fromData(f['custom_data/agent'][()])\n",
    "                self.lastStep = int(f['custom_data/lastStep'][()])\n",
    "                return f['custom_data/extra_data'][()]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25792b9-3eac-4923-852a-7a7442fd1cca",
   "metadata": {},
   "source": [
    "## Test Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5534b50-c80b-40ee-9c0a-11f7baf7de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "agent = AgentBase()\n",
    "b = DeepQLearning(env, agent)\n",
    "b.play(4, 2)\n",
    "\n",
    "getEnvInputOutputShape(env)\n",
    "\n",
    "# import cv2\n",
    "# env.reset()\n",
    "# img = env.render()\n",
    "# img = img[170:-80,:,0] \n",
    "# print(img.shape)\n",
    "# img = cv2.resize(img, (120, 80), interpolation=cv2.INTER_CUBIC) \n",
    "# print(img.dtype)\n",
    "\n",
    "# cv2.imshow('Image', img)\n",
    "# cv2.waitKey(1)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa973c1-d042-4309-a9e8-52c68f45e70d",
   "metadata": {},
   "source": [
    "# <font color='purple'>**Deep Q-Learning CartPole**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a183b8-1e78-457a-af3a-74f2332af290",
   "metadata": {},
   "source": [
    "## Test CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5820d-cd62-4f03-b82f-4dacc43d14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "state_shape = env.observation_space.shape\n",
    "action_count = env.action_space.n\n",
    "print(f'state_shape:{state_shape}, action_count:{action_count}')\n",
    "\n",
    "episodes = 5\n",
    "for e in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "    while not terminated and not truncated:\n",
    "        env.render()\n",
    "        action = random.choice([0, 1])\n",
    "        n_state, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(e, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dfa376",
   "metadata": {},
   "source": [
    "## <font color='green'>DQN_DenseResnet</font> Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cc9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_DenseResnet(DQNBase):\n",
    "    def hiddenLayers(self, inputs):\n",
    "        fx = layers.Flatten()(inputs)\n",
    "        fx = layers.Dense(128)(fx)\n",
    "        fx = bn(fx)\n",
    "        fx = relu(fx)\n",
    "        fx = self.dense_res(fx, 128)\n",
    "        fx = self.dense_res(fx, 128)\n",
    "\n",
    "        advantage = layers.Dense(self.outputShape, activation='linear')(fx)\n",
    "        value = layers.Dense(1, activation='linear')(fx)\n",
    "        q_values = value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))\n",
    "\n",
    "        return q_values\n",
    "\n",
    "    def dense_res(self, x, size):\n",
    "        fx = layers.Dense(size)(x)\n",
    "        fx = bn(fx)\n",
    "        fx = relu(fx)\n",
    "        fx = layers.Dense(size)(fx)\n",
    "        fx = bn(fx)\n",
    "        fx = layers.Add()([fx, x])\n",
    "        fx = relu(fx)\n",
    "        return fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3359d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== Data ==================\n",
    "# releaseMemory()\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "inputFrameCount = 4\n",
    "input_count = env.observation_space.shape[0]\n",
    "action_count = env.action_space.n\n",
    "inputShape=(inputFrameCount, input_count)\n",
    "outputShape=2\n",
    "lr=0.0006\n",
    "decayGamma=0.95\n",
    "exploreRate=[0.01, 1, 0.9996]\n",
    "steps = 80000\n",
    "syncRate=20\n",
    "batchSize=32\n",
    "memSize=1000\n",
    "\n",
    "loss = tf.keras.losses.Huber(delta=0.005)\n",
    "dqn_dense = DQN_DenseResnet(inputShape, outputShape, lr, loss)\n",
    "dqn_dense.summary()\n",
    "agent = DQNAgent(dqn_dense, decayGamma, exploreRate, syncRate)\n",
    "dq_rl = DeepQLearning(env, agent, memSize, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d64cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Training ==============\n",
    "maxScore = 0\n",
    "next_print_time = 0\n",
    "plot = Plot(size=[0,10000, 0, 500], xTitle='Steps', yTitle='Scores', winTitle='CartPole')\n",
    "filePath = '../Data/DenseResnet/data'\n",
    "data = dq_rl.load(filePath)\n",
    "\n",
    "if data is not None:\n",
    "    [plotJson] = data\n",
    "    plot.fromJson(plotJson)\n",
    "\n",
    "def onStepEnd(episode, step, score, terminated):\n",
    "    if step < 2000:\n",
    "        agent.dqn_policy.setLearningRate(0.000001)\n",
    "    elif step < 20000:\n",
    "        agent.dqn_policy.setLearningRate(0.0006)\n",
    "    elif step < 30000:\n",
    "        agent.dqn_policy.setLearningRate(0.0001)\n",
    "    else:\n",
    "        agent.dqn_policy.setLearningRate(0.00005)\n",
    "    \n",
    "    if step > 1000 and step % 1000 == 0:\n",
    "        dq_rl.save(filePath, [plot.toJson()])\n",
    "\n",
    "def onEpisodeEnd(episode, step, score):\n",
    "    global maxScore, next_print_time, plot\n",
    "    maxScore = max(score, maxScore)\n",
    "    plot.add(x=step, y=score)\n",
    "    if time.time() > next_print_time:\n",
    "        next_print_time = time.time() + 2\n",
    "        plot.show(f'[step={step}] a={dq_rl.memory.a} score={score} max={maxScore} lr={agent.dqn_policy.lr} loss={agent.dqn_policy.loss} explor={agent.exploreRate}')\n",
    "    \n",
    "dq_rl.onStepEnd = onStepEnd\n",
    "dq_rl.onEpisodeEnd = onEpisodeEnd\n",
    "dq_rl.play(frameSkipping=inputFrameCount, steps=steps, fromLastStep=True, frameWaitTime=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d93c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Testing ==============\n",
    "agent.isTraining = False\n",
    "plot = Plot(size=[0,0, 0, 1300], xTitle='Steps', yTitle='Scores', winTitle='CartPole')\n",
    "def onEpisodeEnd(episode, step, score):\n",
    "    global maxScore\n",
    "    maxScore = max(score, maxScore)\n",
    "    plot.add(x=step, y=score)\n",
    "    plot.show(f'[step={step}] score={score} max={maxScore}')\n",
    "\n",
    "dq_rl.onStepEnd = None\n",
    "dq_rl.onEpisodeEnd = onEpisodeEnd\n",
    "dq_rl.play(frameSkipping=inputFrameCount, steps=10000, frameWaitTime=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb7d4e",
   "metadata": {},
   "source": [
    "## <font color='green'>DQN_ConvResnet</font> Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef6a46-d753-456a-b39f-8f93a14334d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_ConvResnet(DQNBase):\n",
    "    def hiddenLayers(self, inputs):\n",
    "        fx = conv(inputs, 64, kernel_size=5, strides=2) # (32, 128) -> (16, 64)\n",
    "        fx = bn(fx)\n",
    "        fx = relu(fx)\n",
    "        fx = self.bottleneck(fx, neck_num=16, out_num=64, kernel_size=3)\n",
    "        fx = self.bottleneck(fx, neck_num=16, out_num=64, kernel_size=3)\n",
    "        fx = self.bottleneck(fx, neck_num=32, out_num=128, kernel_size=3, poolStride=2) # (16, 64) -> (8, 32)\n",
    "        fx = self.bottleneck(fx, neck_num=32, out_num=128, kernel_size=3)\n",
    "        fx = self.bottleneck(fx, neck_num=32, out_num=128, kernel_size=3)\n",
    "        fx = self.bottleneck(fx, neck_num=64, out_num=256, kernel_size=3, poolStride=2) # (8, 32) -> (4, 16)\n",
    "        fx = self.bottleneck(fx, neck_num=64, out_num=256, kernel_size=2)\n",
    "        fx = self.bottleneck(fx, neck_num=64, out_num=256, kernel_size=2)\n",
    "        fx = self.bottleneck(fx, neck_num=128, out_num=512, kernel_size=2, poolStride=2) # (4, 16) -> (2, 8)\n",
    "        fx = self.bottleneck(fx, neck_num=128, out_num=512, kernel_size=1)\n",
    "        fx = self.bottleneck(fx, neck_num=128, out_num=512, kernel_size=1)\n",
    "        fx = self.bottleneck(fx, neck_num=256, out_num=1024, kernel_size=2, poolStride=2) # (2, 8) -> (1, 4)\n",
    "        fx = layers.Flatten()(fx)\n",
    "        \n",
    "        fx = layers.Dense(128)(fx)\n",
    "        fx = bn(fx)\n",
    "        fx = relu(fx)\n",
    "        fx = self.dense_res(fx, 128)\n",
    "        fx = self.dense_res(fx, 128)\n",
    "\n",
    "        advantage = layers.Dense(self.outputShape, activation='linear')(fx)\n",
    "        value = layers.Dense(1, activation='linear')(fx)\n",
    "        q_values = value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))\n",
    "\n",
    "        return q_values\n",
    "\n",
    "    def residual_block(self, x, num, kernel_size=3, poolStride=1):\n",
    "        shortcut = x\n",
    "        if poolStride != 1:\n",
    "            shortcut = layers.AveragePooling2D(pool_size=poolStride, strides=poolStride, padding='same')(shortcut)\n",
    "        if num != shortcut.shape[-1]:\n",
    "            shortcut = conv(shortcut, num, kernel_size=1)\n",
    "            shortcut = bn(shortcut)\n",
    "        \n",
    "        fx = conv(x, num, kernel_size=kernel_size, strides=poolStride)\n",
    "        fx = bn_relu(fx)\n",
    "        fx = conv(fx, num, kernel_size=kernel_size)\n",
    "        fx = bn(fx)\n",
    "        fx = layers.Add()([fx, shortcut]) # skip\n",
    "        fx = relu(fx)\n",
    "        return fx\n",
    "        \n",
    "    def bottleneck(self, x, neck_num, out_num, kernel_size=3, poolStride=1):\n",
    "        shortcut = x\n",
    "        if poolStride != 1:\n",
    "            shortcut = layers.AveragePooling2D(pool_size=poolStride, strides=poolStride, padding='same')(shortcut)\n",
    "        if out_num != shortcut.shape[-1]:\n",
    "            shortcut = conv(shortcut, out_num, kernel_size=1)\n",
    "            shortcut = bn(shortcut)\n",
    "        \n",
    "        fx = conv(x, neck_num, kernel_size=1)\n",
    "        fx = bn_relu(fx)\n",
    "        fx = conv(fx, neck_num, kernel_size=kernel_size, strides=poolStride)\n",
    "        fx = bn_relu(fx)\n",
    "        fx = conv(fx, out_num, kernel_size=1)\n",
    "        fx = bn(fx)\n",
    "        fx = layers.Add()([fx, shortcut]) # skip\n",
    "        fx = relu(fx)\n",
    "        return fx\n",
    "        \n",
    "    def dense_res(self, x, num):\n",
    "        input_dim = x.shape[-1]\n",
    "        shortcut = x\n",
    "        \n",
    "        if input_dim != num:\n",
    "            shortcut = layers.Dense(num)(shortcut)\n",
    "            shortcut = bn(shortcut)\n",
    "            \n",
    "        fx = layers.Dense(num)(x)\n",
    "        fx = bn_relu(fx)\n",
    "        fx = layers.Dense(num)(fx)\n",
    "        fx = bn(fx)\n",
    "        fx = layers.Add()([fx, shortcut])\n",
    "        fx = relu(fx)\n",
    "        return fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Training ==============\n",
    "# releaseMemory()\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "inputFrameCount = 4\n",
    "action_count = env.action_space.n\n",
    "imWidth = 128\n",
    "imHeight = 32\n",
    "inputShape=(imHeight, imWidth, inputFrameCount)\n",
    "outputShape=2\n",
    "lr=0.0006\n",
    "decayGamma=0.95\n",
    "exploreRate=[0.02, 1, 0.9999]\n",
    "steps = 500000\n",
    "syncRate=20\n",
    "batchSize=32\n",
    "memSize=20000\n",
    "\n",
    "loss = tf.keras.losses.Huber(delta=1.0)\n",
    "dqn_dense = DQN_ConvResnet(inputShape, outputShape, lr, loss)\n",
    "dqn_dense.summary()\n",
    "agent = DQNAgent(dqn_dense, decayGamma, exploreRate, syncRate)\n",
    "dq_rl = DeepQLearning(env, agent, memSize, batchSize, useImageInput=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxScore = 0\n",
    "next_print_time = 0\n",
    "plot = Plot(size=[0,10000, 0, 300], xTitle='Steps', yTitle='Scores', winTitle='CartPole')\n",
    "filePath = 'CartPole/DQN_ConvResnet_data'\n",
    "data = dq_rl.load(filePath)\n",
    "if data is not None:\n",
    "    [plotJson] = data\n",
    "    plot.fromJson(plotJson)\n",
    "\n",
    "def onInputImage(img):\n",
    "    return cv2.resize(img[170:-80,:,0], (imWidth, imHeight), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def onStepEnd(episode, step, score, terminated):\n",
    "    if step < 2000:\n",
    "        agent.dqn_policy.setLearningRate(0.00000001)\n",
    "    elif step < 10000:\n",
    "        agent.dqn_policy.setLearningRate(0.00001)\n",
    "    elif step < 50000:\n",
    "        agent.dqn_policy.setLearningRate(0.000005)\n",
    "    else:\n",
    "        agent.dqn_policy.setLearningRate(0.000003)\n",
    "\n",
    "    if step > 1000 and (step % 3000 == 0 or step % 100000 == 0):\n",
    "        dq_rl.save(filePath, [plot.toJson()])\n",
    "\n",
    "def onEpisodeEnd(episode, step, score):\n",
    "    global maxScore, next_print_time, plot\n",
    "    maxScore = max(score, maxScore)\n",
    "    if (episode % 10==0):\n",
    "        plot.add(x=step, y=maxScore)\n",
    "        maxScore = 0\n",
    "    if time.time() > next_print_time:\n",
    "        next_print_time = time.time() + 6\n",
    "        plot.show(f'[step={step}] score={score} lr={agent.dqn_policy.lr} loss={agent.dqn_policy.loss} explor={agent.exploreRate}')\n",
    "\n",
    "def onKeyPressed(key):\n",
    "    global lr\n",
    "    if key == ord('w'):\n",
    "        lr = lr * 10\n",
    "    elif key == ord('s'):\n",
    "        lr = lr * 0.1\n",
    "    agent.dqn_policy.setLearningRate(lr)\n",
    "\n",
    "dq_rl.onInputImage = onInputImage\n",
    "dq_rl.onStepEnd = onStepEnd\n",
    "dq_rl.onEpisodeEnd = onEpisodeEnd\n",
    "dq_rl.onKeyPressed = onKeyPressed\n",
    "dq_rl.play(frameSkipping=inputFrameCount, steps=steps, fromLastStep=True, frameWaitTime=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c286929",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.isTraining = False\n",
    "maxScore = 0\n",
    "sumScore = 0\n",
    "def onEpisodeEnd(episode, step, score):\n",
    "    global maxScore, sumScore\n",
    "    maxScore = max(score, maxScore)\n",
    "    sumScore += score\n",
    "    print(f'[step={step}] score={score} max={maxScore} mean={sumScore/episode}')\n",
    "\n",
    "dq_rl.onStepEnd = None\n",
    "dq_rl.onEpisodeEnd = onEpisodeEnd\n",
    "dq_rl.play(frameSkipping=inputFrameCount, steps=5000, frameWaitTime=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f48764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "class TemporalMemory:\n",
    "    def __init__(self, maxlen, chunkSize=100):\n",
    "        self.chunkSize = chunkSize\n",
    "        self.maxlen = maxlen\n",
    "        self.array = np.empty(maxlen, dtype=object)  # Initialize with a numpy array of objects\n",
    "        self.tail = 0\n",
    "        self.size = 0\n",
    "\n",
    "        chunkCount = math.ceil(self.maxlen/self.chunkSize)\n",
    "        self.savedChunks = [True] * chunkCount\n",
    "        self.cache_batch = None\n",
    "\n",
    "    def push(self, value):\n",
    "        if self.size < self.maxlen:\n",
    "            self.size += 1\n",
    "        \n",
    "        chunkIndex = math.floor(self.tail/self.chunkSize)\n",
    "        self.savedChunks[chunkIndex] = False\n",
    "\n",
    "        self.array[self.tail] = value\n",
    "        self.tail = (self.tail + 1) % self.maxlen\n",
    "\n",
    "    def sampleBatch(self, batchSize):\n",
    "        if self.size == self.maxlen:\n",
    "            array = self.array\n",
    "        else:\n",
    "            array = self.array[0:self.tail]\n",
    "        \n",
    "        if (batchSize <= len(array)):\n",
    "            batch = np.random.choice(array, batchSize, replace=False)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if self.cache_batch is None:\n",
    "            self.cache_batch = np.stack(batch, axis=0)\n",
    "        else:\n",
    "            np.stack(batch, axis=0, out=self.cache_batch)\n",
    "\n",
    "        return self.cache_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def save(self, filePath):\n",
    "        h5_path = f'{filePath}.h5'\n",
    "\n",
    "        with h5py.File(h5_path, 'a') as f:\n",
    "            if 'metadata' in f:\n",
    "                [chunkSize, maxlen] = f['metadata'][()]\n",
    "                if chunkSize != self.chunkSize:\n",
    "                    raise Exception('[TemporalMemory] currrent chunkSize = {self.chunkSize} and file chunkSize = {chunkSize} are different')\n",
    "                if maxlen != self.maxlen:\n",
    "                    raise Exception('[TemporalMemory] currrent maxlen = {self.maxlen} and file maxlen = {maxlen} are different')\n",
    "                f['metadata'][()] = [self.chunkSize, self.maxlen, self.tail, self.size]\n",
    "            else:\n",
    "                f['metadata'] = [self.chunkSize, self.maxlen, self.tail, self.size]\n",
    "\n",
    "            if 'array_chunks' in f:\n",
    "                chunks = f['array_chunks']\n",
    "            else:\n",
    "                chunks = f.create_dataset('array_chunks', shape=len(self.savedChunks), dtype=h5py.string_dtype(encoding='utf-8'), chunks=1)\n",
    "\n",
    "            for i in range(0, len(self.savedChunks)):\n",
    "                if not self.savedChunks[i]:\n",
    "                    self.savedChunks[i] = True\n",
    "                    start = i * self.chunkSize\n",
    "                    end = min(start + self.chunkSize, self.maxlen)\n",
    "                    chunk = self.array[start:end]\n",
    "                    jsonStr = json.dumps(chunk.tolist())\n",
    "                    chunks[i] = jsonStr\n",
    "\n",
    "    def load(self, filePath):\n",
    "        h5_path = f'{filePath}.h5'\n",
    "        if not os.path.exists(h5_path):\n",
    "            print(f'[TemporalMemory] fail to load filePath=\"{filePath}\", file does not exist')\n",
    "            return\n",
    "        \n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            [self.chunkSize, self.maxlen, self.tail, self.size] = f['metadata'][()]\n",
    "\n",
    "            if len(self.array) != self.maxlen:\n",
    "                self.array = np.empty(self.maxlen, dtype=object)\n",
    "\n",
    "            chunks = f['array_chunks']\n",
    "            self.savedChunks = [True] * len(chunks)\n",
    "            for i in range(len(chunks)):\n",
    "                jsonStr = chunks[i].decode('utf-8')\n",
    "                if jsonStr == '':\n",
    "                    continue\n",
    "                start = i * self.chunkSize\n",
    "                end = min(start + self.chunkSize, self.maxlen)\n",
    "                self.array[start:end] = json.loads(jsonStr)\n",
    "\n",
    "\n",
    "mem1 = TemporalMemory(19,90)\n",
    "mem1.load('test')\n",
    "print(mem1.sampleBatch(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
